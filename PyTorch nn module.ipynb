{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: nn\n",
    "\n",
    "A fully connected ReLU network with one hidden layer, trained to predict y from x by minimizing squared Eucledian distance \n",
    "\n",
    "Note:\n",
    "PyTorch autograd makes it easy to define computational graphs and take gradients, but raw autograd can be a bit too low-level for defining complex neural networks; this is where the nn package can help. The nn package defines a set of Modules, which are in a way neural network layers that produce output from input and may have some trainable weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 64\n",
    "input_dimension = 1000\n",
    "hidden_dimension = 100\n",
    "output_dimension = 10\n",
    "\n",
    "x = torch.randn(batch_size, input_dimension)\n",
    "y = torch.randn(batch_size, output_dimension)\n",
    "\n",
    "#Use the nn package to define our model as a sequence of layers\n",
    "#nn.Sequential is a Module which contains other Modules, and a applies them in sequence\n",
    "#to produce its output. Each Linear Module computes output form input using a linear function,\n",
    "#and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_dimension, hidden_dimension),\n",
    "    torch.nn.ReLU(), \n",
    "    torch.nn.Linear(hidden_dimension, output_dimension),\n",
    ")\n",
    "\n",
    "#The nn package also contains definitions of populat loss functions; in this case\n",
    "# we will use Mean Squared Error (MSE) as our loss function\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "for n in range(500):\n",
    "    #Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When \n",
    "    # doing so you pass a Tensor of input data to the Module and it produces a Tensor of output data\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    #Compute and print loss. We pass Tensors containing the predicted and true values of y, \n",
    "    #and the loss function returns a Tensor containing the loss \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(n, loss.item())\n",
    "    \n",
    "    #Zero the gradients before running the backward pass\n",
    "    model.zero_grad()\n",
    "    \n",
    "    #Backward pass: compute gradient of the loss with respect to all the learnable \n",
    "    #parameters of the model. Internally, the parameters of each Module are stored in Tensors\n",
    "    #with required_grad = True, so this call will compute gradients for all learnable parameters \n",
    "    # in the model and print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
