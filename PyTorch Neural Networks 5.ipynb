{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow: Static Graphs\n",
    "\n",
    "PyTorch autograd looks a lot like TensorFlow: in both frameworks we define a computational graph, and use automatic differentiation to compute gradients. The biggest difference between the two is that TensorFlow ='s computational graphs are *static* and PyTorch uses *dynamic* computational graphs\n",
    "\n",
    "In TensorFlow, we define the computational graph once then execute the same graph over and over again possibly feeding different input data to the graph. In PyTorch, each forward pass defines a new computational graph. \n",
    "\n",
    "Static graphs are nice because you can optimize the graph up frontl for example a framework might decide to fuse some graph operations for efficience, or to come up with a strategy for distributing the graph across many GPUs or many machines. If you are reusing the same graph over and over the this potentially costly up front optimization can be amortized as the same grapg is rerun over and over. \n",
    "\n",
    "Ine aspect where static and dynamic graohs differ is control flow. For some models we may wish to perform different computation for each data point; for example a recurrent network might be unrolled for different numbers of time steps for each data point; this unrolling can be implemented as a loop. With a static graph the loop construct needs to be a part of the graph; for this reason TensorFlow provides operations such as tf.scan for embedding loops into the graph. With dynamic graphs the situation is simpler: since we build graphs on the fly for each example, we can use normal imperative flow control to perform computation that differs dor each input. \n",
    "\n",
    "To contrast with the PyTorch autograd example committed previously, here we use TensorFlow to fit a simple two-layer net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38111696.0\n",
      "36732130.0\n",
      "37021190.0\n",
      "32755602.0\n",
      "25652180.0\n",
      "21555562.0\n",
      "15462438.0\n",
      "9580216.0\n",
      "5501326.0\n",
      "3127626.8\n",
      "1877278.2\n",
      "1225871.5\n",
      "874920.9\n",
      "671038.8\n",
      "541408.7\n",
      "451354.72\n",
      "384103.06\n",
      "331183.03\n",
      "288067.66\n",
      "252198.92\n",
      "221976.14\n",
      "196160.86\n",
      "174002.75\n",
      "154844.19\n",
      "138198.34\n",
      "123665.11\n",
      "110922.22\n",
      "99707.19\n",
      "89810.64\n",
      "81056.945\n",
      "73318.49\n",
      "66431.35\n",
      "60287.72\n",
      "54796.273\n",
      "49876.383\n",
      "45459.406\n",
      "41489.555\n",
      "37912.203\n",
      "34683.47\n",
      "31767.062\n",
      "29125.812\n",
      "26731.418\n",
      "24558.738\n",
      "22587.277\n",
      "20792.37\n",
      "19156.37\n",
      "17663.379\n",
      "16299.621\n",
      "15052.058\n",
      "13909.186\n",
      "12861.594\n",
      "11900.797\n",
      "11018.814\n",
      "10209.779\n",
      "9465.892\n",
      "8780.506\n",
      "8149.1284\n",
      "7566.9746\n",
      "7029.796\n",
      "6533.708\n",
      "6075.248\n",
      "5651.412\n",
      "5259.451\n",
      "4896.5684\n",
      "4560.5186\n",
      "4249.5205\n",
      "3961.3206\n",
      "3693.97\n",
      "3445.8787\n",
      "3215.5454\n",
      "3001.5747\n",
      "2802.7188\n",
      "2618.0635\n",
      "2446.0996\n",
      "2286.1157\n",
      "2137.1987\n",
      "1998.5311\n",
      "1869.2654\n",
      "1748.8064\n",
      "1636.5515\n",
      "1531.8401\n",
      "1434.1288\n",
      "1342.969\n",
      "1257.8574\n",
      "1178.3678\n",
      "1104.1436\n",
      "1034.8031\n",
      "970.09283\n",
      "909.5482\n",
      "852.91125\n",
      "799.932\n",
      "750.41516\n",
      "704.0683\n",
      "660.6957\n",
      "620.11163\n",
      "582.10815\n",
      "546.51276\n",
      "513.1854\n",
      "481.95038\n",
      "452.69458\n",
      "425.287\n",
      "399.71954\n",
      "375.76288\n",
      "353.2635\n",
      "332.15198\n",
      "312.34976\n",
      "293.76746\n",
      "276.3214\n",
      "259.9408\n",
      "244.56625\n",
      "230.12753\n",
      "216.56874\n",
      "203.82787\n",
      "191.85193\n",
      "180.60208\n",
      "170.0339\n",
      "160.10608\n",
      "150.76826\n",
      "141.9827\n",
      "133.72513\n",
      "125.95498\n",
      "118.647804\n",
      "111.77929\n",
      "105.317856\n",
      "99.23576\n",
      "93.5103\n",
      "88.12517\n",
      "83.05603\n",
      "78.28634\n",
      "73.79376\n",
      "69.564026\n",
      "65.58916\n",
      "61.840588\n",
      "58.310295\n",
      "54.985016\n",
      "51.852436\n",
      "48.90315\n",
      "46.124184\n",
      "43.50476\n",
      "41.03681\n",
      "38.71221\n",
      "36.52308\n",
      "34.457237\n",
      "32.512566\n",
      "30.687801\n",
      "28.96782\n",
      "27.345984\n",
      "25.815575\n",
      "24.373024\n",
      "23.011036\n",
      "21.72732\n",
      "20.516418\n",
      "19.374783\n",
      "18.297033\n",
      "17.280674\n",
      "16.322289\n",
      "15.4165745\n",
      "14.562126\n",
      "13.755648\n",
      "12.995316\n",
      "12.277657\n",
      "11.599291\n",
      "10.958674\n",
      "10.354324\n",
      "9.78373\n",
      "9.244776\n",
      "8.736101\n",
      "8.255792\n",
      "7.801693\n",
      "7.3733215\n",
      "6.9684896\n",
      "6.5858316\n",
      "6.225315\n",
      "5.884101\n",
      "5.562225\n",
      "5.2582283\n",
      "4.9704494\n",
      "4.699067\n",
      "4.4422517\n",
      "4.200182\n",
      "3.9710078\n",
      "3.75462\n",
      "3.5499618\n",
      "3.3564277\n",
      "3.1737256\n",
      "3.0011208\n",
      "2.8380928\n",
      "2.6839132\n",
      "2.5382593\n",
      "2.4008384\n",
      "2.2704444\n",
      "2.147574\n",
      "2.0311742\n",
      "1.92104\n",
      "1.8170745\n",
      "1.7186899\n",
      "1.6259239\n",
      "1.5379479\n",
      "1.4549301\n",
      "1.3763766\n",
      "1.302129\n",
      "1.231866\n",
      "1.1655035\n",
      "1.1028817\n",
      "1.0435014\n",
      "0.98729765\n",
      "0.9341926\n",
      "0.883979\n",
      "0.8364308\n",
      "0.79154336\n",
      "0.74904484\n",
      "0.7087711\n",
      "0.6707684\n",
      "0.63475156\n",
      "0.60085607\n",
      "0.5685965\n",
      "0.53813034\n",
      "0.5093756\n",
      "0.48212114\n",
      "0.4563166\n",
      "0.4319787\n",
      "0.408874\n",
      "0.38699377\n",
      "0.3663438\n",
      "0.34679884\n",
      "0.32830697\n",
      "0.31087345\n",
      "0.294299\n",
      "0.2786217\n",
      "0.26378825\n",
      "0.24976079\n",
      "0.23642975\n",
      "0.2238743\n",
      "0.21197838\n",
      "0.20070079\n",
      "0.19005808\n",
      "0.17993614\n",
      "0.17040893\n",
      "0.16135013\n",
      "0.15280314\n",
      "0.1446963\n",
      "0.13705172\n",
      "0.12979083\n",
      "0.12289482\n",
      "0.11643794\n",
      "0.110289596\n",
      "0.10447074\n",
      "0.09891929\n",
      "0.09369248\n",
      "0.088743016\n",
      "0.08405954\n",
      "0.079614066\n",
      "0.07545084\n",
      "0.07143532\n",
      "0.067677826\n",
      "0.06411937\n",
      "0.060735986\n",
      "0.057544358\n",
      "0.054510858\n",
      "0.051645655\n",
      "0.04894059\n",
      "0.046365768\n",
      "0.04391681\n",
      "0.041624058\n",
      "0.039439254\n",
      "0.037355226\n",
      "0.03539839\n",
      "0.033566035\n",
      "0.031822413\n",
      "0.030172344\n",
      "0.028588668\n",
      "0.027084429\n",
      "0.025663966\n",
      "0.02433096\n",
      "0.02306158\n",
      "0.021858156\n",
      "0.020730786\n",
      "0.019650623\n",
      "0.01864212\n",
      "0.017664382\n",
      "0.016755316\n",
      "0.015892994\n",
      "0.015074734\n",
      "0.014306481\n",
      "0.013562072\n",
      "0.012871198\n",
      "0.012216759\n",
      "0.011589177\n",
      "0.010987403\n",
      "0.010441692\n",
      "0.009908668\n",
      "0.0094033815\n",
      "0.0089177415\n",
      "0.008476565\n",
      "0.008041836\n",
      "0.007632956\n",
      "0.0072541474\n",
      "0.0068900418\n",
      "0.006544982\n",
      "0.0062145926\n",
      "0.005907909\n",
      "0.0056220917\n",
      "0.0053404463\n",
      "0.0050813504\n",
      "0.0048298175\n",
      "0.004595625\n",
      "0.0043714717\n",
      "0.0041594277\n",
      "0.003959599\n",
      "0.0037697633\n",
      "0.0035911384\n",
      "0.003417782\n",
      "0.0032532364\n",
      "0.0030995547\n",
      "0.0029545806\n",
      "0.0028210748\n",
      "0.0026854915\n",
      "0.002561762\n",
      "0.0024458235\n",
      "0.0023351894\n",
      "0.0022284924\n",
      "0.002127551\n",
      "0.0020309812\n",
      "0.0019406674\n",
      "0.0018545005\n",
      "0.0017709284\n",
      "0.001694849\n",
      "0.0016213144\n",
      "0.0015534204\n",
      "0.0014860111\n",
      "0.0014221631\n",
      "0.0013629377\n",
      "0.0013064006\n",
      "0.0012508333\n",
      "0.0011987342\n",
      "0.0011492062\n",
      "0.0011003125\n",
      "0.0010573622\n",
      "0.0010141643\n",
      "0.0009726955\n",
      "0.00093383976\n",
      "0.0008972949\n",
      "0.0008622307\n",
      "0.00082857686\n",
      "0.000796634\n",
      "0.0007670751\n",
      "0.0007377786\n",
      "0.00071148586\n",
      "0.0006826903\n",
      "0.00065750454\n",
      "0.0006356026\n",
      "0.0006112561\n",
      "0.0005889797\n",
      "0.0005680816\n",
      "0.00054739974\n",
      "0.00052930845\n",
      "0.000509797\n",
      "0.0004911791\n",
      "0.00047360364\n",
      "0.00045833923\n",
      "0.0004423723\n",
      "0.00042839246\n",
      "0.00041392513\n",
      "0.000401047\n",
      "0.00038816192\n",
      "0.00037533778\n",
      "0.000362965\n",
      "0.00035148606\n",
      "0.00034024531\n",
      "0.00032953577\n",
      "0.00031848546\n",
      "0.00030939459\n",
      "0.00029925312\n",
      "0.0002902676\n",
      "0.0002820092\n",
      "0.00027424417\n",
      "0.0002657366\n",
      "0.00025765982\n",
      "0.0002504295\n",
      "0.0002434729\n",
      "0.00023660342\n",
      "0.0002303133\n",
      "0.00022334022\n",
      "0.00021695958\n",
      "0.0002112904\n",
      "0.00020617706\n",
      "0.00019993701\n",
      "0.00019494316\n",
      "0.00018939571\n",
      "0.00018439125\n",
      "0.00017956615\n",
      "0.0001744288\n",
      "0.00017033026\n",
      "0.0001652514\n",
      "0.0001603688\n",
      "0.00015652251\n",
      "0.00015248246\n",
      "0.00014915103\n",
      "0.00014491324\n",
      "0.00014199286\n",
      "0.00013789133\n",
      "0.0001342356\n",
      "0.00013090286\n",
      "0.00012774466\n",
      "0.00012491242\n",
      "0.0001223063\n",
      "0.00011885961\n",
      "0.00011645217\n",
      "0.00011398302\n",
      "0.00011050926\n",
      "0.00010795921\n",
      "0.00010563794\n",
      "0.00010337886\n",
      "0.000100895755\n",
      "9.904142e-05\n",
      "9.678058e-05\n",
      "9.4765295e-05\n",
      "9.216237e-05\n",
      "9.052822e-05\n",
      "8.87475e-05\n",
      "8.6889995e-05\n",
      "8.4857355e-05\n",
      "8.307763e-05\n",
      "8.135136e-05\n",
      "7.964536e-05\n",
      "7.820216e-05\n",
      "7.6478056e-05\n",
      "7.4756426e-05\n",
      "7.3626245e-05\n",
      "7.194853e-05\n",
      "7.099118e-05\n",
      "6.94102e-05\n",
      "6.816605e-05\n",
      "6.703561e-05\n",
      "6.5684246e-05\n",
      "6.434751e-05\n",
      "6.313807e-05\n",
      "6.208125e-05\n",
      "6.0875576e-05\n",
      "5.9970836e-05\n",
      "5.8901962e-05\n",
      "5.736662e-05\n",
      "5.6347395e-05\n",
      "5.5095516e-05\n",
      "5.3929696e-05\n",
      "5.3349017e-05\n",
      "5.23292e-05\n",
      "5.1554056e-05\n",
      "5.080556e-05\n",
      "4.9892944e-05\n",
      "4.9042475e-05\n",
      "4.8092006e-05\n",
      "4.7471673e-05\n",
      "4.6356035e-05\n",
      "4.5688103e-05\n",
      "4.5055087e-05\n",
      "4.4181048e-05\n",
      "4.3710566e-05\n",
      "4.299541e-05\n",
      "4.2460615e-05\n",
      "4.1762123e-05\n",
      "4.1146028e-05\n",
      "4.048722e-05\n",
      "3.989447e-05\n",
      "3.9332168e-05\n",
      "3.8766622e-05\n",
      "3.8401e-05\n",
      "3.766155e-05\n",
      "3.7163238e-05\n",
      "3.6580535e-05\n",
      "3.6090532e-05\n",
      "3.5686713e-05\n",
      "3.5105513e-05\n",
      "3.4403485e-05\n",
      "3.394154e-05\n",
      "3.364621e-05\n",
      "3.315905e-05\n",
      "3.2655007e-05\n",
      "3.22279e-05\n",
      "3.1765467e-05\n",
      "3.127054e-05\n",
      "3.0784842e-05\n",
      "3.040377e-05\n",
      "3.0017545e-05\n",
      "2.9537638e-05\n",
      "2.9339259e-05\n",
      "2.8975082e-05\n",
      "2.8474826e-05\n",
      "2.8335202e-05\n",
      "2.8052213e-05\n",
      "2.7735543e-05\n",
      "2.741851e-05\n",
      "2.6951984e-05\n",
      "2.6684243e-05\n",
      "2.633626e-05\n",
      "2.5959827e-05\n",
      "2.5774792e-05\n",
      "2.5331929e-05\n",
      "2.5047288e-05\n",
      "2.4657009e-05\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#First we setup the computational graph\n",
    "\n",
    "batch_size = 64\n",
    "input_dimension = 1000\n",
    "hidden_dimension = 100\n",
    "output_dimension = 10\n",
    "\n",
    "# Create placeholders for the input and target data; \n",
    "# these will be filled with real data when we execute the graph\n",
    "x = tf.placeholder(tf.float32, shape=(None, input_dimension))\n",
    "y = tf.placeholder(tf.float32, shape=(None, output_dimension))\n",
    "\n",
    "#Create variables for the weights and initialize them with random data \n",
    "# A tf.Variable persists its value across executions of the graph\n",
    "weight1 = tf.Variable(tf.random_normal((input_dimension, hidden_dimension)))\n",
    "weight2 = tf.Variable(tf.random_normal((hidden_dimension, output_dimension)))\n",
    "\n",
    "# Forward pass: set up the computational graph that will execute numeric operations\n",
    "h = tf.matmul(x, weight1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, weight2)\n",
    "\n",
    "#Loss computation using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "#Compute gradient of the loss with respect to weight1 and weight2\n",
    "grad_weight1, grad_weight2 = tf.gradients(loss, [weight1, weight2])\n",
    "\n",
    "#Update the weights using gradient descent. Tp actually update the weights we need to\n",
    "# evaluate new_weight1 and new_weight2 when executing the graph. Note that in TenorFlow\n",
    "#the act of updating the value of the weights is part of the computational graph;\n",
    "# in PyTorch this happens outside of the computational graph \n",
    "learning_rate = 1e-6\n",
    "new_weight1 = weight1.assign(weight1 - learning_rate * grad_weight1)\n",
    "new_weight2 = weight2.assign(weight2 - learning_rate * grad_weight2)\n",
    "\n",
    "# Now we have built our computational graph, so we enter a TensorFlow session \n",
    "# to actually execute the graph\n",
    "with tf.Session() as sess:\n",
    "    #Run the graph once to initialize the Variables weight1 and weight2\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Create numpy arrays holding the actual data for the inputs x and targets y\n",
    "    x_value = np.random.randn(batch_size, input_dimension)\n",
    "    y_value = np.random.randn(batch_size, output_dimension)\n",
    "    for _ in range(500):\n",
    "        #Execute the graph many times. Each time it executes we want to bind \n",
    "        #x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "        #Each time we execute the graph we want to compute the values for loss\n",
    "        # new_weight1 and new_weight2; the values of these Tensors are returned as numpy arrays\n",
    "        loss_value, _, _ = sess.run([loss, new_weight1, new_weight2], \n",
    "                                   feed_dict = {x: x_value, y: y_value})\n",
    "        print(loss_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
