{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Control Flow + Weight Sharing\n",
    "\n",
    "To showcase the power of PyTorch dynamic graphs, we will implement a very strange model: a fully connected ReLU network that on each forward pass randomly chooses a number between 1 and 4 and has that many hidden layers, reusing the same weights multipl tomes to compute the innermost hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch \n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_dimension, output_dimension):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use \n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(input_dimension, hidden_dimension)\n",
    "        self.middle_linear = torch.nn.Linear(hidden_dimension, hidden_dimension)\n",
    "        self.output_linear = torch.nn.Linear(hidden_dimension, output_dimension)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3 \n",
    "        and reuse the middle_linear Module that many times to compute hidden layer \n",
    "        representations.\n",
    "        \n",
    "        Since each forward pass build a dynamic computation graph, we can use normal\n",
    "        Pythin control-flow operators like loops or condotional statements when \n",
    "        defining the forward pass of the model.\n",
    "        \n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once. \n",
    "        \"\"\"\n",
    "        \n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0,3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "batch_size = 64\n",
    "input_dimension = 1000\n",
    "hidden_dimension = 100\n",
    "output_dimension = 10\n",
    "\n",
    "#Create random Tensors to hole inputs and outputs \n",
    "x = torch.randn(batch_size, input_dimension)\n",
    "y = torch.randn(batch_size, output_dimension)\n",
    "\n",
    "#Construct our model instantiating the class defined above \n",
    "model = DynamicNet(input_dimension, hidden_dimension, output_dimension)\n",
    "\n",
    "#Contruct our loss funciton and an Optimizer. Training this strange model with \n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum = 0.9)\n",
    "for n in range(500):\n",
    "    #Forward pass: Compute predicted y by passing x to the model \n",
    "    y_pred = model(x)\n",
    "    \n",
    "    #Compute and print loss \n",
    "    loss = criterion(y_pred, y)\n",
    "    print(n, loss.item())\n",
    "    \n",
    "    #Zero gradients, perfomr a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
